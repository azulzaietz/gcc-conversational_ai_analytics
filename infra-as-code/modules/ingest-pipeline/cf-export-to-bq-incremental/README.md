## Create the CCAI tables
The CCAI Insights export process has a limit of 100000 conversations per export. This is why we need to implement an incremental load process.
We will be working with 2 tables: 
- `export_staging`: this table will contain the new/changed conversations generated since the last load
- `export`: this table will contain the full history of conversations.

The `export_staging` table can be generated by creating an empty table in BigQuery and manually running the Export process from the CCAI Insights console. This will generate the table schema of the `export_staging` table.

Now that we have the `export_staging` table, we can create the `export` table by running the following DDL in BigQuery:

```sql
CREATE TABLE mydataset.export
LIKE mydataset.export_staging
```

As new fields may have been added over time as part of the schema evolution of CCAI Insights, we need to make sure the 'merge query' available in `function-source-code/lib.py` matches the schema of the `export` table.

The incremental load process is based on the field `conversationUpdateTimestampUtc` which gets updated each time the conversation is updated.

## Add clustering to the export table
```sh
gcloud config set project <your-project-id>
bq update --clustering_fields=conversationName mydataset.export
```

## Terraform variables

| name | description | type | required | default | example |
|---|---|:---:|:---:|:---:|:---:|
|project_id|Project ID in which the resources will be provisioned|`string`|Yes|||
|region|Region in which the resources will be provisioned|`string`|Yes||`us-central1`|
|ccai_insights_project_id|Project ID of CCAI Insights|`string`|Yes|||
|ccai_insights_location_id|Location ID of CCAI Insights|`string`|Yes||`global`|
|bigquery_project_id|Project ID to which we will be sending the CCAI Insights data to BigQuery|`string`|Yes|||
|bigquery_staging_dataset|BigQuery dataset in which we will be writing the Staging data|`string`|Yes||`mydatasetname`|
|bigquery_final_dataset|BigQuery dataset in which we will be writing the data|`string`|Yes||`mydatasetname`|
|bigquery_staging_table|BigQuery table in which we will be writing the Staging data|`string`|Yes||`mytablename`|
|bigquery_final_table|BigQuery table in which we will be writing the data|`string`|Yes||`mytablename`|
|export_to_bq_cron|CRON expression that defines how often the CCAI Insights data will be exported|`string`|Yes||`0 * * * *`|
|service_account_email|Service Account used as identity by the Cloud Function|`string`|Yes||`ccai-insights-demo1@gsd-ccai-insights-offering.iam.gserviceaccount.com`|
|cf_bucket_name|Bucket name to use for storing the Cloud Function bundle|`string`|Yes||`my-cloudfunction-bucket`|
|function_name|Cloud Function name|`string`|Yes||`export-to-bq-incremental`|